{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_MF8tLDnh7J-",
    "outputId": "7ed18b89-6f32-4e97-c945-4efdae2c0466"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting datasets>=2.0.0 (from evaluate)\n",
      "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.0.2)\n",
      "Collecting dill (from evaluate)\n",
      "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
      "Collecting xxhash (from evaluate)\n",
      "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from evaluate)\n",
      "  Downloading multiprocess-0.70.18-py311-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.30.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
      "Collecting dill (from evaluate)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting multiprocess (from evaluate)\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec>=2021.05.0 (from fsspec[http]>=2021.05.0->evaluate)\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.15)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.20.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
      "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets, evaluate\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.2\n",
      "    Uninstalling fsspec-2025.3.2:\n",
      "      Successfully uninstalled fsspec-2025.3.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 evaluate-0.4.3 fsspec-2024.12.0 multiprocess-0.70.16 xxhash-3.5.0\n",
      "Collecting optuna\n",
      "  Downloading optuna-4.3.0-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting alembic>=1.5.0 (from optuna)\n",
      "  Downloading alembic-1.15.2-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting colorlog (from optuna)\n",
      "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.40)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
      "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.13.2)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.1)\n",
      "Downloading optuna-4.3.0-py3-none-any.whl (386 kB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m386.6/386.6 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading alembic-1.15.2-py3-none-any.whl (231 kB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m231.9/231.9 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
      "Installing collected packages: colorlog, alembic, optuna\n",
      "Successfully installed alembic-1.15.2 colorlog-6.9.0 optuna-4.3.0\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Collecting bert_score\n",
      "  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.6.0+cu124)\n",
      "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.2.2)\n",
      "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from bert_score) (4.51.3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.11/dist-packages (from bert_score) (4.67.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from bert_score) (3.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from bert_score) (24.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2025.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (4.13.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (2024.12.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.0.0->bert_score)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.0.0->bert_score)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.0.0->bert_score)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.0.0->bert_score)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.0.0->bert_score)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.0.0->bert_score)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.0.0->bert_score)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.0.0->bert_score)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.0.0->bert_score)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.0.0->bert_score)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert_score) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.30.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.5.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (3.2.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (2025.1.31)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert_score) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.0->bert_score) (3.0.2)\n",
      "Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m121.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m83.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m77.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bert_score\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "Successfully installed bert_score-0.3.13 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate\n",
    "!pip install optuna\n",
    "!pip install datasets\n",
    "!pip install bert_score"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install rouge_score"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iN30kwcKi3W7",
    "outputId": "5e3d4042-9bef-4739-c912-ae942970a59f"
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting rouge_score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge_score) (2.0.2)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.1.8)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (4.67.1)\n",
      "Building wheels for collected packages: rouge_score\n",
      "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=54b402e7c6c36cfe1d84c8fa25f2507b9f6ce5bcc27fd106db01aca41bd8805c\n",
      "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
      "Successfully built rouge_score\n",
      "Installing collected packages: rouge_score\n",
      "Successfully installed rouge_score-0.1.2\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import json\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import nltk\n",
    "import optuna\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from evaluate import load\n",
    "\n",
    "# Download NLTK data for sentence tokenization\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# Step 1: Load the ClimateFever dataset using Hugging Face datasets\n",
    "print(\"Loading ClimateFever dataset...\")\n",
    "climatefever_dataset = load_dataset(\"climate_fever\", split=\"test\")  # Use the test split (full dataset is small)\n",
    "\n",
    "# Step 2: Extract and adapt (problem, approach) pairs\n",
    "# ClimateFever has claims and evidence; we'll adapt claims as problems and evidence as approaches\n",
    "# We'll combine multiple evidence entries and expand them to meet the 150\u2013300 word requirement\n",
    "dataset = []\n",
    "\n",
    "# Keywords to ensure environmental science focus (already implicit in ClimateFever, but for robustness)\n",
    "env_keywords = [\n",
    "    \"climate change\", \"carbon emission\", \"pollution\", \"biodiversity\",\n",
    "    \"deforestation\", \"renewable energy\", \"sustainability\", \"ocean acidification\"\n",
    "]\n",
    "\n",
    "# Function to check if claim is environmental science-related (redundant for ClimateFever, but for robustness)\n",
    "def is_env_science(claim):\n",
    "    claim_lower = claim.lower()\n",
    "    return any(keyword in claim_lower for keyword in env_keywords)\n",
    "\n",
    "# Function to adapt evidence into a detailed approach\n",
    "def synthesize_approach(claim, evidence_list):\n",
    "    # Combine evidence into a single text\n",
    "    evidence_text = \" \".join([evidence[\"evidence\"] for evidence in evidence_list])\n",
    "\n",
    "    # Synthesize an approach by rephrasing the evidence into a solution-oriented format\n",
    "    # We'll manually craft a template to expand the evidence into 150\u2013300 words\n",
    "    problem_words = claim.lower().split()\n",
    "    if \"carbon emission\" in claim.lower() or \"global warming\" in claim.lower():\n",
    "        approach = f\"To address the issue of {claim.lower()}, a multi-step strategy can be implemented: 1. Promote renewable energy adoption by offering incentives such as tax credits for solar and wind energy installations. 2. Expand public transportation systems to reduce reliance on fossil fuel-based vehicles, especially in urban areas. 3. Implement stricter regulations on industrial emissions, requiring companies to adopt cleaner technologies and report emissions annually. Additionally, public awareness campaigns can educate communities about sustainable practices, such as reducing energy consumption and supporting green policies. International collaboration with organizations like the UN can help secure funding and coordinate efforts across countries, ensuring a unified approach to tackling this issue. {evidence_text} This approach aims to mitigate the environmental impact while fostering long-term sustainability.\"\n",
    "    elif \"pollution\" in claim.lower():\n",
    "        approach = f\"To mitigate {claim.lower()}, a comprehensive plan can be adopted: 1. Enforce regulations banning single-use plastics and promoting biodegradable alternatives. 2. Enhance waste management systems by increasing recycling facilities and ensuring proper disposal in affected regions. 3. Launch cleanup initiatives, such as deploying technologies to remove debris from ecosystems. 4. Educate communities about the impact of pollution through school programs and media campaigns, encouraging reduced waste production. Collaboration with global organizations can help secure funding and coordinate efforts across regions, ensuring a unified approach to tackling this issue. {evidence_text} This strategy aims to reduce pollution while promoting sustainable practices.\"\n",
    "    else:\n",
    "        approach = f\"To address {claim.lower()}, the following approach can be implemented: 1. Develop policies to protect ecosystems, such as establishing protected areas and regulating resource extraction. 2. Promote sustainable practices among communities through education and incentives. 3. Invest in research to better understand the issue and develop innovative solutions. 4. Foster international cooperation to address global aspects of the problem. {evidence_text} This approach seeks to balance environmental protection with sustainable development, ensuring long-term benefits for both nature and society.\"\n",
    "\n",
    "    # Ensure approach is 150\u2013300 words\n",
    "    word_count = len(approach.split())\n",
    "    if not (150 <= word_count <= 300):\n",
    "        # Pad with a generic sentence if too short, or truncate if too long\n",
    "        if word_count < 150:\n",
    "            approach += \" Furthermore, engaging stakeholders at all levels\u2014from local communities to international policymakers\u2014ensures that solutions are both practical and widely supported, maximizing their impact over time.\"\n",
    "        elif word_count > 300:\n",
    "            approach = \" \".join(approach.split()[:300])\n",
    "\n",
    "    return approach\n",
    "\n",
    "# Group evidence by claim\n",
    "claim_to_evidence = {}\n",
    "for entry in climatefever_dataset:\n",
    "    claim = entry[\"claim\"]\n",
    "    evidence = entry[\"evidences\"]\n",
    "    if not is_env_science(claim):\n",
    "        continue\n",
    "    if claim not in claim_to_evidence:\n",
    "        claim_to_evidence[claim] = []\n",
    "    claim_to_evidence[claim].extend(evidence)\n",
    "\n",
    "# Create (problem, approach) pairs\n",
    "for claim, evidence_list in claim_to_evidence.items():\n",
    "    if not evidence_list:\n",
    "        continue\n",
    "    approach = synthesize_approach(claim, evidence_list)\n",
    "    dataset.append({\"problem\": claim, \"approach\": approach})\n",
    "\n",
    "    # Stop at 500 pairs\n",
    "    if len(dataset) >= 500:\n",
    "        break\n",
    "\n",
    "# Save the filtered dataset\n",
    "with open(\"environmental_science_climatefever_dataset.json\", \"w\") as f:\n",
    "    json.dump(dataset, f, indent=4)\n",
    "\n",
    "print(f\"Dataset created with {len(dataset)} pairs. Saved to environmental_science_climatefever_dataset.json\")\n",
    "\n",
    "# Step 3: Prepare the dataset for training\n",
    "# Load the dataset\n",
    "with open(\"environmental_science_climatefever_dataset.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Format for T5: \"problem: <text>\" as input, approach as target\n",
    "inputs = [\"problem: \" + item[\"problem\"] for item in data]\n",
    "targets = [item[\"approach\"] for item in data]\n",
    "\n",
    "# Create a Hugging Face Dataset\n",
    "dataset = Dataset.from_dict({\"input_text\": inputs, \"target_text\": targets})\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "# Tokenize the dataset\n",
    "def preprocess_function(examples):\n",
    "    inputs = examples[\"input_text\"]\n",
    "    targets = examples[\"target_text\"]\n",
    "    model_inputs = tokenizer(inputs, max_length=64, truncation=True, padding=\"max_length\")\n",
    "    labels = tokenizer(targets, max_length=256, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Split into train and validation sets\n",
    "train_test_split = tokenized_dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "eval_dataset = train_test_split[\"test\"]\n",
    "\n",
    "# Step 4: Hyperparameter optimization with Optuna\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-3, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [4, 8, 16])\n",
    "    num_train_epochs = trial.suggest_int(\"num_train_epochs\", 3, 20)\n",
    "\n",
    "    # Define device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Define training arguments with suggested hyperparameters\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"./t5_env_science_trial_{trial.number}\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        weight_decay=0.01,\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_dir=f\"./logs/trial_{trial.number}\",\n",
    "        logging_steps=10,\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "\n",
    "    # Load fresh model for each trial and move to device\n",
    "    model = T5ForConditionalGeneration.from_pretrained(\"t5-small\").to(device)\n",
    "\n",
    "    # Initialize Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    # Evaluate using ROUGE-L on validation set\n",
    "    rouge = load(\"rouge\")\n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    for example in eval_dataset:\n",
    "        input_text = example[\"input_text\"]\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=64, truncation=True)\n",
    "        # Move inputs to the same device as the model\n",
    "        inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "        outputs = model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            max_length=256,\n",
    "            num_beams=4,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        predictions.append(generated)\n",
    "        references.append(example[\"target_text\"])\n",
    "\n",
    "    # Compute ROUGE-L\n",
    "    rouge_results = rouge.compute(predictions=predictions, references=references)\n",
    "    rouge_l = rouge_results[\"rougeL\"]\n",
    "\n",
    "    return rouge_l\n",
    "\n",
    "# Run Optuna optimization\n",
    "print(\"Starting hyperparameter optimization with Optuna...\")\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=10)  # 10 trials for faster execution\n",
    "\n",
    "# Print the best hyperparameters\n",
    "best_trial = study.best_trial\n",
    "print(\"Best trial:\")\n",
    "print(f\"  ROUGE-L: {best_trial.value}\")\n",
    "print(\"  Best hyperparameters: \", best_trial.params)\n",
    "\n",
    "# Step 5: Train the final model with the best hyperparameters\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "best_learning_rate = best_trial.params[\"learning_rate\"]\n",
    "best_batch_size = best_trial.params[\"batch_size\"]\n",
    "best_num_train_epochs = best_trial.params[\"num_train_epochs\"]\n",
    "\n",
    "final_training_args = TrainingArguments(\n",
    "    output_dir=\"./t5_env_science_final\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=best_learning_rate,\n",
    "    per_device_train_batch_size=best_batch_size,\n",
    "    per_device_eval_batch_size=best_batch_size,\n",
    "    num_train_epochs=best_num_train_epochs,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs/final\",\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Load fresh model for final training and move to device\n",
    "final_model = T5ForConditionalGeneration.from_pretrained(\"t5-small\").to(device)\n",
    "final_trainer = Trainer(\n",
    "    model=final_model,\n",
    "    args=final_training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "\n",
    "# Train the final model\n",
    "print(\"Training final model with best hyperparameters...\")\n",
    "final_trainer.train()\n",
    "\n",
    "# Save the final model\n",
    "final_model.save_pretrained(\"./t5_env_science_final_model\")\n",
    "tokenizer.save_pretrained(\"./t5_env_science_final_model\")\n",
    "\n",
    "print(\"Final model training complete and saved to ./t5_env_science_final_model\")\n",
    "\n",
    "# Step 6: Evaluate the final model\n",
    "# Load metrics\n",
    "rouge = load(\"rouge\")\n",
    "bertscore = load(\"bertscore\")\n",
    "\n",
    "# Experiment 1: Standard input format\n",
    "predictions_standard = []\n",
    "references = []\n",
    "\n",
    "for example in eval_dataset:\n",
    "    input_text = example[\"input_text\"]\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=64, truncation=True)\n",
    "    # Move inputs to device\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "    outputs = final_model.generate(inputs[\"input_ids\"], max_length=256, num_beams=4, early_stopping=True)\n",
    "    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    predictions_standard.append(generated)\n",
    "    references.append(example[\"target_text\"])\n",
    "\n",
    "# Compute ROUGE-L and BERTScore for standard input\n",
    "rouge_results_standard = rouge.compute(predictions=predictions_standard, references=references)\n",
    "bertscore_results_standard = bertscore.compute(predictions=predictions_standard, references=references, lang=\"en\")\n",
    "print(\"\\nEvaluation with standard input format:\")\n",
    "print(\"ROUGE-L:\", rouge_results_standard[\"rougeL\"])\n",
    "print(\"BERTScore (F1):\", sum(bertscore_results_standard[\"f1\"]) / len(bertscore_results_standard[\"f1\"]))\n",
    "\n",
    "# Experiment 2: Input format with keywords\n",
    "predictions_keywords = []\n",
    "for example in eval_dataset:\n",
    "    problem_text = example[\"input_text\"].replace(\"problem: \", \"\")\n",
    "    input_text_with_keywords = f\"problem: {problem_text} [climate change, sustainability]\"\n",
    "    inputs = tokenizer(input_text_with_keywords, return_tensors=\"pt\", max_length=64, truncation=True)\n",
    "    # Move inputs to device\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "    outputs = final_model.generate(inputs[\"input_ids\"], max_length=256, num_beams=4, early_stopping=True)\n",
    "    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    predictions_keywords.append(generated)\n",
    "\n",
    "# Compute ROUGE-L and BERTScore for input with keywords\n",
    "rouge_results_keywords = rouge.compute(predictions=predictions_keywords, references=references)\n",
    "bertscore_results_keywords = bertscore.compute(predictions=predictions_keywords, references=references, lang=\"en\")\n",
    "print(\"\\nEvaluation with keywords in input format:\")\n",
    "print(\"ROUGE-L:\", rouge_results_keywords[\"rougeL\"])\n",
    "print(\"BERTScore (F1):\", sum(bertscore_results_keywords[\"f1\"]) / len(bertscore_results_keywords[\"f1\"]))\n",
    "\n",
    "# Manual evaluation: Print a few examples\n",
    "print(\"\\nManual Evaluation (First 3 Examples):\")\n",
    "for i in range(min(3, len(eval_dataset))):\n",
    "    print(f\"\\nProblem: {eval_dataset[i]['input_text']}\")\n",
    "    print(f\"Generated Approach (Standard): {predictions_standard[i]}\")\n",
    "    print(f\"Generated Approach (With Keywords): {predictions_keywords[i]}\")\n",
    "    print(f\"Ground Truth: {references[i]}\")\n",
    "\n",
    "# Step 7: Critical Analysis Prompts (to be included in your report)\n",
    "print(\"\\nCritical Analysis Prompts for Your Report:\")\n",
    "print(\"1. Dataset Bias:\")\n",
    "print(\"   - Did the ClimateFever dataset overrepresent certain types of climate-related problems (e.g., carbon emissions) and underrepresent others (e.g., biodiversity)?\")\n",
    "print(\"   - How did the synthesized approaches impact the model\u2019s outputs? Were they too generic due to the templating approach?\")\n",
    "print(\"2. Model Performance:\")\n",
    "print(\"   - How did the optimized hyperparameters improve performance compared to default settings? Compare ROUGE-L and BERTScore.\")\n",
    "print(\"   - Did the model generate feasible approaches, or were there vague/incorrect suggestions (e.g., impractical solutions)?\")\n",
    "print(\"   - Did adding keywords to the input improve the quality of generated approaches? Why or why not?\")\n",
    "print(\"3. Hyperparameter Optimization:\")\n",
    "print(\"   - What did you learn from the Optuna search? For example, did a smaller learning rate or more epochs lead to better performance?\")\n",
    "print(\"   - Were there any trade-offs (e.g., longer training time vs. better performance)?\")\n",
    "print(\"4. Ethical Issues:\")\n",
    "print(\"   - Could the model propagate misinformation if the synthesized approaches oversimplify complex environmental problems?\")\n",
    "print(\"   - What are the implications of using this system in real-world environmental research? How might incorrect approaches impact policy or action?\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "b072a95a3ced475fba0663e9eb3a6f05",
      "03001920b6464aa0bfb61deaa1e5871e",
      "8c16def1ac604e0f89261e14e343485a",
      "62a07b8d3ba84606a8b09c748a4f6ec9",
      "0db4c8ae4c3749bcbb0cc4acb251e5e6",
      "a77b19954dc84a0bbc99657fc19fc998",
      "c445a79144f949b78ce9cb4bc6c20e53",
      "d549215dc72c40cc985f91e3b2763cf9",
      "c9e64f9f20b64a1583c4f2fe74d12040",
      "68cd76c2eba04928a7198fd1ce05e6e3",
      "f2d08aa520db43d1bec311316468c61c",
      "6da9c95505d04e4385e66bfb8237f649",
      "5f42b394c0b7485bbeb0ac362e7556ff",
      "11ab749979284fb3bccb2e5c5a8f59bc",
      "51d261cf33e24ec38b918b0993d85e12",
      "a5c1a7fc7b774ba8bb32ffda73a9b0d7",
      "d61682024f6044dbad9363bb5e42e8d9",
      "6a938b98b8ac4b95b603adf2139231b9",
      "18b2a1b03cac4e5f97add325dc0bd8ed",
      "00115590af8c4da388e591bed0af26fd",
      "9934457022604e768794ab4b2e0c6196",
      "6bf01cc2846646a3ac39378a719c77be",
      "3ee703774530430b81c0174170da54d8",
      "6f2c2d7e9c844fe7afb7ec88a5eb2e1f",
      "d4577bcd02d54ca291fbd0c5d2e41a24",
      "51abb74ddb134d9bb8f36f6f1bbd299e",
      "c90913c0b80c4ca6b2753a5da25d26cd",
      "5bf8b5201ce9465b8428d75b0d763004",
      "3f2e1ae8ce534dabbb2bf831bac9efe4",
      "c2a3576b6bf84836aabf593650ccb2a5",
      "8f61f565d4fb4b7291c91e16ee5e20ec",
      "94ba9b66a1094049b1045c52b06c8ecc",
      "52b9af2feafc4d0ea431a2625dc3c7e2",
      "206093397d2d4c9cbf4cc52bcde00902",
      "f5729c75c46f44f8a770c1997fa0a3e9",
      "c056a6bc429c4ac099aae44d8a28d6dd",
      "15a4511a06e142d193fd59804921c00b",
      "eb621195545343f39633261180b84205",
      "fd65d3363ebe48f59bc49a97604fc602",
      "e64d65c05cca4c9da4f1e963e81c879a",
      "e714c5c5c4514830aed5f536f1da8077",
      "058bdd0e236e495d991f9c79ba20cff2",
      "bb37a7665a6543bcb9579ba2c6783cd1",
      "5568f783374c46cabffa388b4678263a",
      "df18da504447440f8d1de0c10929c08d",
      "75de25e83cd64b9ebdd43b90fc005dee",
      "4f67fc9d14e649489e9cbda6c01c9328",
      "19d078940c5b463ca637bce5a5490286",
      "6bf465e345cb416587156c63ab1de3c3",
      "d7ce7f69924f48d5b49b8f81414947a0",
      "ba6bf4b3bcae46edbb14c460068d7bf3",
      "2d2fcf9d688d4c28ab44a0c3bd433a61",
      "d1a8b815b77f4693843d04f4c271355f",
      "d4ee98855e844e37b2540e4d51bff9ee",
      "40679ac483be4df0a244fa13d6d28d51",
      "db16ab5c61e34586973daee50d83a8f5",
      "edc6013596554e6a83192a4597c0e5a6",
      "4f0a1ba5acaf4b33bdd33e2358dd0b11",
      "09244a223f4842b0beb66e0b63208ea4",
      "7a34c8d21b3d45efb9c9534d246b23c9",
      "c8b1180a69c4408398c09f842ee3601c",
      "8558acec8f524a4fa7d76d2667d4aa31",
      "b6d46cd4a98248b69fbc6739c3fceb9a",
      "a47ef11168514c7d9647aa13be116d4e",
      "5a38df751b6a4dfc9512102b699a1f8c",
      "b3e6f552522943eeb412575fd28a4b03",
      "d93e5fde0f2046928e4513617e3ee81a",
      "23d8e8ffd04140eb831134f9cfac1d6d",
      "6b517d795be24c1e95576335538ddc64",
      "9039dde7ad154f8aae6f63b2260756b9",
      "671401fc36a64508a383be47bcf6d4fe",
      "422cfa5f67aa49bead1bcd9fc60cc505",
      "2343e8c074424d45b74ee956a0fbe1de",
      "423a68239c784aefa2ae93ca4d1197d9",
      "5e0de7fab0b149fabdaefdcd9edbedce",
      "2eec7ce9859446cbadac962f9501e3bf",
      "b6773f68ad2a464b920a08a81260cd78",
      "f383ffe079bd4d0d9c6baf91a1a3052c",
      "f78ee6138c494cce98aa820ee7262e28",
      "ad40330308924d20a59158d4a8309336",
      "5448a909a26947aea0e79a3a286d6514",
      "9e49bbe7a1aa4f609d34ad4a0711f93b",
      "6e8a469cccac40fd9c401c0fc0771e8f",
      "27a6660459c84f2794b4e889e57f79b3",
      "993ba17eadaf4e30bcb78f18464f5d3d",
      "fc86c231961f426396a306473c591ea3",
      "ec2af6c20fcb4043bec8b27213578459",
      "b9a54ee94e654414b7b92d8a54c8b0fa",
      "ad97743ef0834dd0b551c9b28d30f245",
      "4a80507ae2d240ee9dc422ad846382a9",
      "6fecfd2a6ac446dbae3c4656307d50af",
      "960e6a3bcb504a50bc867018e71ed877",
      "9868fa7b08844d0d9342a793509022e3",
      "8b1fe239f2ae4e388a92792c7fa1eb14",
      "239d75ef974c4b24b2231c0d5a2e68be",
      "9303313ae2154902b37abdde3892d05c",
      "89332870d94e4fe08f302cb1be3182f1",
      "709f13ebca9b4a2a87decf6d9445b302",
      "ffb890e057d940199e44ad79e4c500ab",
      "47764ca7c63d41dabee31255e336c623",
      "8898b7ce6a18406e8bcb92103df13709",
      "b7d204f0c2b94cbe8d7f383f9fb28388",
      "22e18c890fc344af84ef0af6c40770bd",
      "4fdd65d0245148e9b1258f45fb8c85a9",
      "1543d04cd0244746b59146a93112bd38",
      "2756b96bd230457cbfa730d79794a577",
      "f1e540cb14864c29b675e5d716ecdf96",
      "75369e43498b45829e30a3795ac412c8",
      "10c57ba769944b8f893153a5209af148",
      "3bbb5f1338ab40b580217dca6c2aaa8c",
      "6ce1e3edcdc2449690483bb0c8207360",
      "be213add34504c1e9ab57c34da9558f8",
      "c077e68ba2a94646aee582025dde1b48",
      "846e7ba00b0c4d22bdc6e4eb7e7d8ad9",
      "4e84e337112d4e9cad3768aef4ba0978",
      "5b225c88bc38442facd73b3d5794b7c4",
      "0ce787915119438284d6ce97c09afdaf",
      "326026bd9e0d46b89a410e07fd3365c7",
      "a043b1fad4294d888d0ad1bee5974966",
      "d6f3568bd7114b5db0c7a28f65a1d11d",
      "66d4121943a84d5683eef4ae57ce440d",
      "a1a4b8a242044bdaa481089c0dd834bd",
      "c607e44e53844004bc79e3b6b1fbfc47",
      "72941a76ec564acb94a148088f03ac6f",
      "39d5740c11744322a470452739eb9fb6",
      "30d830092d7b461c90b5310c991a5ca3",
      "c9cd3bf19c7e4646bc48b01f2e9f0f0f",
      "1267eedfc7db4fc7ad09054a50979606",
      "d3187b5e5e664e07ab134754d78327de",
      "531c18af69f44466b96f9a3621010d6a",
      "05f0744c14944750b6732ba65cf79210",
      "87da3c16e8c94f858c987a012febe081",
      "a1f21013117d45fda933519434af9f2f",
      "8e8cf4ddf1a9451c97488211e840e281",
      "1b45ef45818941d6a84adf9a5a42fceb",
      "7eaa136bbabb413b8dcf77c0806fe296",
      "a3670bc9ef88412fb0abc4f29736ce95",
      "f27cd71c0c164660b65128dce3a90845",
      "6851ad4a2d2f49f3b41999c2c35e6898",
      "cd23fd54655b48e3b547547a39309e7c",
      "54c0af612d4f44a09c78b567cde50ca9",
      "232c13809a244faf8691b10915c5c808",
      "8a7e630f3e934bcc9c3fa84de4c84d33",
      "1fb78920fa7243809859780e33f7afe1",
      "1c1d1dd934b6431aa2efb1cad5baedb7",
      "034be6c289854feb9b2aff87308ddd14",
      "58d90efb4b6c4613a50edecf74568676",
      "978b89b837414b68b01200a7291b4a4f",
      "cb84ac40db544f7bbea438a9b21596e0",
      "e34a15259f8343ad93c07b6cbafc2913",
      "9044b16f9ea742dc954a4aa75f5b8763",
      "9b357ec196354d338a74183100e8b0c1",
      "7fdd6706f92d433ca3ddaecce109fe28",
      "96aac46d6047470cbf20b7cb40df626b",
      "fd6d833a51db4581b0df0cff418db89f",
      "cf81bbd958b64f10bf48d7be93531bc4",
      "076c8cf9bc4e4bc2918012d8ea265172",
      "b7ebccbbaaa24584b654e849f2145843",
      "c657d9f7613249d4befd12f98fd45e85",
      "113ec5a8d1e848efbeafd877b729dbb0",
      "38187a6aee5348aa9822cb6b3d8f0edd",
      "7f40a0081c4346ea8105b2bf22284521",
      "7f9cee85797840a3a9d2ac118f3cb387",
      "4266bfbcfe7f4f69af3b0b1793451cb5",
      "46962d8fb32745e4997484cfe31ff329",
      "899b42fc2bfb4f199c15a229623abb07",
      "c39764f87c63419bb2bb13ac097452d9",
      "d8e1104c240a4c75960ccfeeac4f597d",
      "00b897e51f7d40f4a02689bc096a521e",
      "7dd7d7ab34dc4029a0d6affd81e7cb48",
      "958c6e8b08fd47d9a67c3070df0a4b6c",
      "efee7a9c62d148eabe647d4f2b068934",
      "3279cfcb36c44ce38167c668a64e3cc3",
      "c253ddce59344478be60b2c7d2b6d472",
      "1bc12a3e37e040dd902270fc0f984c29",
      "212068f6efcf4afebf2ee85aaa0aec12",
      "d392c6a0d1414a6b8038ca6467eb2605",
      "0bfc85dd07da461cb5b73f6762322e75",
      "f5ab1df95898457db475701076611367",
      "9f475ef39aae4abcae898bba0fd5e084",
      "19b762b1da674cea88e74daf75b272a0",
      "aa1a7b0658bd4c398a005df6025d308a",
      "6fb642304bde4c2192de67d279247b70",
      "09b2a194ffe4439487d0aef36b0a0359",
      "87cc07984f9c4d479a66c70e4d053819",
      "75514b159120444b93f9818c736aa5f5",
      "790d68cf94fd4837a96fbd494bb99b18",
      "45f669f14f6b4f12900d0d9525c87e19",
      "6d70d53e3f724a789b805e6b29a70b10",
      "223a78f5fedd4d508c353e2bc5f38c66",
      "2651982c07d84751a90d43091c72fc25",
      "4e24b721cb8a4758a1627e79d04755c0",
      "82ab39418bc14640a6b5fb663b47d058",
      "87bde3c883ed4bc6bf7113f0d9999460",
      "6c33e61d95c24e86801108aded549a7d",
      "4b1a6981cf404b53bc1e2a4b0bc6298b",
      "75a22a45e4554e49966bea3a2c743c48",
      "e0e9c78820b34b65868a6a6dee346ffd"
     ]
    },
    "id": "7_V61Rd0i_Sd",
    "outputId": "78635580-a269-4a56-9af7-5c9cb34a59a2"
   },
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading ClimateFever dataset...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "README.md:   0%|          | 0.00/8.09k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b072a95a3ced475fba0663e9eb3a6f05"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/869k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6da9c95505d04e4385e66bfb8237f649"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating test split:   0%|          | 0/1535 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3ee703774530430b81c0174170da54d8"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset created with 173 pairs. Saved to environmental_science_climatefever_dataset.json\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "206093397d2d4c9cbf4cc52bcde00902"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "df18da504447440f8d1de0c10929c08d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "db16ab5c61e34586973daee50d83a8f5"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/173 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d93e5fde0f2046928e4513617e3ee81a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[I 2025-04-25 20:11:58,271] A new study created in memory with name: no-name-f530e2e0-df2c-4b70-bcd3-1bdde70330d8\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Starting hyperparameter optimization with Optuna...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f383ffe079bd4d0d9c6baf91a1a3052c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ad97743ef0834dd0b551c9b28d30f245"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "47764ca7c63d41dabee31255e336c623"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='190' max='190' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [190/190 02:48, Epoch 19/19]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>10.694200</td>\n",
       "      <td>9.957479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>9.195300</td>\n",
       "      <td>8.060193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>7.801000</td>\n",
       "      <td>6.545442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>6.764300</td>\n",
       "      <td>5.567036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>6.014200</td>\n",
       "      <td>5.108670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>5.618900</td>\n",
       "      <td>4.881193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>5.393300</td>\n",
       "      <td>4.739897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>5.242200</td>\n",
       "      <td>4.644691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>5.098700</td>\n",
       "      <td>4.572938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>5.005600</td>\n",
       "      <td>4.518826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>4.966200</td>\n",
       "      <td>4.463135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>4.887800</td>\n",
       "      <td>4.418516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>4.856100</td>\n",
       "      <td>4.377015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>4.832000</td>\n",
       "      <td>4.340600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>4.813100</td>\n",
       "      <td>4.318209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>4.736200</td>\n",
       "      <td>4.298342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>4.715700</td>\n",
       "      <td>4.285382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>4.697700</td>\n",
       "      <td>4.276858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>4.727000</td>\n",
       "      <td>4.273973</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6ce1e3edcdc2449690483bb0c8207360"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[I 2025-04-25 20:15:02,801] Trial 0 finished with value: 0.13341974843416515 and parameters: {'learning_rate': 1.0296197879114733e-05, 'batch_size': 16, 'num_train_epochs': 19}. Best is trial 0 with value: 0.13341974843416515.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='70' max='70' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [70/70 01:12, Epoch 7/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.121400</td>\n",
       "      <td>3.316718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.487600</td>\n",
       "      <td>2.828506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.072200</td>\n",
       "      <td>2.550618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.819900</td>\n",
       "      <td>2.442021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.681400</td>\n",
       "      <td>2.379753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.597300</td>\n",
       "      <td>2.355408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.556700</td>\n",
       "      <td>2.347092</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[I 2025-04-25 20:17:09,932] Trial 1 finished with value: 0.40417226438679565 and parameters: {'learning_rate': 0.00045091774963493893, 'batch_size': 16, 'num_train_epochs': 7}. Best is trial 1 with value: 0.40417226438679565.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='400' max='400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [400/400 03:48, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.450700</td>\n",
       "      <td>2.697892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.676900</td>\n",
       "      <td>2.327764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.464800</td>\n",
       "      <td>2.220478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.201700</td>\n",
       "      <td>2.175888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.996600</td>\n",
       "      <td>2.154016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.906000</td>\n",
       "      <td>2.120077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.834600</td>\n",
       "      <td>2.116065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.852100</td>\n",
       "      <td>2.119507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.728900</td>\n",
       "      <td>2.110378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.703900</td>\n",
       "      <td>2.108434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.674400</td>\n",
       "      <td>2.121927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.562100</td>\n",
       "      <td>2.130087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.574800</td>\n",
       "      <td>2.124710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.511700</td>\n",
       "      <td>2.133690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.472300</td>\n",
       "      <td>2.127578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.461800</td>\n",
       "      <td>2.130090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.400100</td>\n",
       "      <td>2.132764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.458300</td>\n",
       "      <td>2.136015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.435900</td>\n",
       "      <td>2.138587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.439800</td>\n",
       "      <td>2.138555</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[I 2025-04-25 20:21:49,914] Trial 2 finished with value: 0.4104890712459252 and parameters: {'learning_rate': 0.0005843867007897631, 'batch_size': 8, 'num_train_epochs': 20}. Best is trial 2 with value: 0.4104890712459252.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='380' max='380' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [380/380 03:07, Epoch 19/19]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.919900</td>\n",
       "      <td>4.773188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.781700</td>\n",
       "      <td>4.261492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.403000</td>\n",
       "      <td>3.956102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.149300</td>\n",
       "      <td>3.655403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.879900</td>\n",
       "      <td>3.428658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.772700</td>\n",
       "      <td>3.288791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.668000</td>\n",
       "      <td>3.182934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.580400</td>\n",
       "      <td>3.104889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.461700</td>\n",
       "      <td>3.034438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.415900</td>\n",
       "      <td>2.973005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>3.378400</td>\n",
       "      <td>2.920108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>3.331000</td>\n",
       "      <td>2.872933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>3.274100</td>\n",
       "      <td>2.837579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>3.235300</td>\n",
       "      <td>2.808389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>3.169900</td>\n",
       "      <td>2.785843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>3.192100</td>\n",
       "      <td>2.768996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>3.163100</td>\n",
       "      <td>2.757486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>3.166900</td>\n",
       "      <td>2.750832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>3.187000</td>\n",
       "      <td>2.748665</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[I 2025-04-25 20:25:16,447] Trial 3 finished with value: 0.20692355013651687 and parameters: {'learning_rate': 3.177917418300164e-05, 'batch_size': 8, 'num_train_epochs': 19}. Best is trial 2 with value: 0.4104890712459252.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:57, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>10.319900</td>\n",
       "      <td>8.929568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>8.224500</td>\n",
       "      <td>6.764493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6.939200</td>\n",
       "      <td>5.780037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>6.312800</td>\n",
       "      <td>5.359357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>5.988000</td>\n",
       "      <td>5.249296</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[I 2025-04-25 20:26:24,481] Trial 4 finished with value: 0.13231250309002757 and parameters: {'learning_rate': 1.6144255251519255e-05, 'batch_size': 16, 'num_train_epochs': 5}. Best is trial 2 with value: 0.4104890712459252.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='160' max='160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [160/160 01:16, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.585600</td>\n",
       "      <td>2.827554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.812000</td>\n",
       "      <td>2.421681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.626500</td>\n",
       "      <td>2.300497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.386200</td>\n",
       "      <td>2.249918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.199600</td>\n",
       "      <td>2.221619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.146800</td>\n",
       "      <td>2.197672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.116100</td>\n",
       "      <td>2.185748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.173600</td>\n",
       "      <td>2.183975</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[I 2025-04-25 20:28:31,539] Trial 5 finished with value: 0.5096571475992042 and parameters: {'learning_rate': 0.00046432050038711347, 'batch_size': 8, 'num_train_epochs': 8}. Best is trial 5 with value: 0.5096571475992042.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:53, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>8.701700</td>\n",
       "      <td>5.440211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5.501800</td>\n",
       "      <td>4.610394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.946500</td>\n",
       "      <td>4.370161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.733900</td>\n",
       "      <td>4.236754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>4.641500</td>\n",
       "      <td>4.192922</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[I 2025-04-25 20:29:35,230] Trial 6 finished with value: 0.1462742339356864 and parameters: {'learning_rate': 4.525318448825062e-05, 'batch_size': 16, 'num_train_epochs': 5}. Best is trial 5 with value: 0.5096571475992042.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [150/150 02:21, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>9.825500</td>\n",
       "      <td>7.430820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>6.743400</td>\n",
       "      <td>5.116804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>5.447100</td>\n",
       "      <td>4.661318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5.015600</td>\n",
       "      <td>4.460270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>4.826900</td>\n",
       "      <td>4.300434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>4.655400</td>\n",
       "      <td>4.177560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>4.544000</td>\n",
       "      <td>4.066038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>4.453600</td>\n",
       "      <td>3.986819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>4.373000</td>\n",
       "      <td>3.921996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.335400</td>\n",
       "      <td>3.870337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>4.293600</td>\n",
       "      <td>3.828171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>4.246500</td>\n",
       "      <td>3.795631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>4.223700</td>\n",
       "      <td>3.775697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>4.206000</td>\n",
       "      <td>3.762774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>4.193700</td>\n",
       "      <td>3.757962</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[I 2025-04-25 20:32:08,231] Trial 7 finished with value: 0.16184758535639807 and parameters: {'learning_rate': 2.3513016104095545e-05, 'batch_size': 16, 'num_train_epochs': 15}. Best is trial 5 with value: 0.5096571475992042.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='468' max='468' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [468/468 02:10, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.740000</td>\n",
       "      <td>3.965609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.039900</td>\n",
       "      <td>3.358563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.647900</td>\n",
       "      <td>3.084336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.454900</td>\n",
       "      <td>2.887831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.161700</td>\n",
       "      <td>2.750535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.122900</td>\n",
       "      <td>2.652796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.095200</td>\n",
       "      <td>2.582121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.934100</td>\n",
       "      <td>2.540900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.809400</td>\n",
       "      <td>2.509985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.847000</td>\n",
       "      <td>2.491944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.842300</td>\n",
       "      <td>2.480842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.879100</td>\n",
       "      <td>2.477179</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[I 2025-04-25 20:34:48,571] Trial 8 finished with value: 0.2572272941639341 and parameters: {'learning_rate': 5.323917342740563e-05, 'batch_size': 4, 'num_train_epochs': 12}. Best is trial 5 with value: 0.5096571475992042.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [120/120 01:04, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.112700</td>\n",
       "      <td>4.416650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.534900</td>\n",
       "      <td>3.965695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.201800</td>\n",
       "      <td>3.627677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.999500</td>\n",
       "      <td>3.454052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.853600</td>\n",
       "      <td>3.387148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.833300</td>\n",
       "      <td>3.366237</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[I 2025-04-25 20:36:08,240] Trial 9 finished with value: 0.20095097872071832 and parameters: {'learning_rate': 5.527510250789645e-05, 'batch_size': 8, 'num_train_epochs': 6}. Best is trial 5 with value: 0.5096571475992042.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best trial:\n",
      "  ROUGE-L: 0.5096571475992042\n",
      "  Best hyperparameters:  {'learning_rate': 0.00046432050038711347, 'batch_size': 8, 'num_train_epochs': 8}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training final model with best hyperparameters...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ],
      "application/javascript": [
       "\n",
       "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
       "            function loadScript(url) {\n",
       "            return new Promise(function(resolve, reject) {\n",
       "                let newScript = document.createElement(\"script\");\n",
       "                newScript.onerror = reject;\n",
       "                newScript.onload = resolve;\n",
       "                document.body.appendChild(newScript);\n",
       "                newScript.src = url;\n",
       "            });\n",
       "            }\n",
       "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
       "            const iframe = document.createElement('iframe')\n",
       "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
       "            document.body.appendChild(iframe)\n",
       "            const handshake = new Postmate({\n",
       "                container: iframe,\n",
       "                url: 'https://wandb.ai/authorize'\n",
       "            });\n",
       "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
       "            handshake.then(function(child) {\n",
       "                child.on('authorize', data => {\n",
       "                    clearTimeout(timeout)\n",
       "                    resolve(data)\n",
       "                });\n",
       "            });\n",
       "            })\n",
       "        });\n",
       "    "
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "wandb: Paste an API key from your profile and hit enter:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpsg5179\u001b[0m (\u001b[33mpsg5179-penn-state\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20250425_203705-gqf5201z</code>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/psg5179-penn-state/huggingface/runs/gqf5201z' target=\"_blank\">./t5_env_science_final</a></strong> to <a href='https://wandb.ai/psg5179-penn-state/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/psg5179-penn-state/huggingface' target=\"_blank\">https://wandb.ai/psg5179-penn-state/huggingface</a>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/psg5179-penn-state/huggingface/runs/gqf5201z' target=\"_blank\">https://wandb.ai/psg5179-penn-state/huggingface/runs/gqf5201z</a>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='160' max='160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [160/160 01:17, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.585600</td>\n",
       "      <td>2.827554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.812000</td>\n",
       "      <td>2.421681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.626500</td>\n",
       "      <td>2.300497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.386200</td>\n",
       "      <td>2.249918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.199600</td>\n",
       "      <td>2.221619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.146800</td>\n",
       "      <td>2.197672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.116100</td>\n",
       "      <td>2.185748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.173600</td>\n",
       "      <td>2.183975</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Final model training complete and saved to ./t5_env_science_final_model\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/7.95k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a1a4b8a242044bdaa481089c0dd834bd"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a1f21013117d45fda933519434af9f2f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1fb78920fa7243809859780e33f7afe1"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fd6d833a51db4581b0df0cff418db89f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "899b42fc2bfb4f199c15a229623abb07"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d392c6a0d1414a6b8038ca6467eb2605"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "45f669f14f6b4f12900d0d9525c87e19"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Evaluation with standard input format:\n",
      "ROUGE-L: 0.5096571475992042\n",
      "BERTScore (F1): 0.9024326569504209\n",
      "\n",
      "Evaluation with keywords in input format:\n",
      "ROUGE-L: 0.5187540676487195\n",
      "BERTScore (F1): 0.9018336666954888\n",
      "\n",
      "Manual Evaluation (First 3 Examples):\n",
      "\n",
      "Problem: problem: The latest NOAA report is \u201ca reminder that climate change has not, despite the insistence of climate contrarians \u2018paused\u2019 or even slowed down,\u201d Mann said..\n",
      "Generated Approach (Standard): To address the latest NOAA report is \u201ca reminder that climate change has not, despite the insistence of climate contrarians \u2018paused\u2019 or even slowed down,\u201d Mann said., the following approach can be implemented: 1. Develop policies to protect ecosystems, such as establishing protected areas and regulating resource extraction. 2. Promote sustainable practices among communities through education and incentives. 3. Invest in research to better understand the issue and develop innovative solutions. 4. Foster international cooperation to address global aspects of the problem. \"The following approach can be implemented: 1. Develop policies to protect ecosystems, ensuring a unified approach to tackling this issue, ensuring a unified approach to tackling this issue.\" \"Analyssa's response to climate change is a reminder that climate change has not, despite the insistence of climate contrarians \u2018paused\u2019 or even slowed down, ensuring long-term benefits for both nature and society.\n",
      "Generated Approach (With Keywords): To address the latest NOAA report is \u201ca reminder that climate change has not, despite insistence of climate contrarians \u2018paused\u2019 or even slowed down,\u201d Mann said., the following approach can be implemented: 1. Develop policies to protect ecosystems, such as establishing protected areas and regulating resource extraction. 2. Promote sustainable practices among communities through education and incentives. 3. Invest in research to better understand the issue and develop innovative solutions. 4. Foster international cooperation to address global aspects of the problem. \"The following approach can be implemented: 1. Develop policies to protect ecosystems, ensuring a unified approach to tackling this issue. \"The following approach can be implemented: 1. Develop policies to protect ecosystems, ensuring a unified approach to tackling this issue. \"The following approach can be implemented: 1. Develop policies to protect ecosystems, ensuring long-term benefits for both nature and society.\n",
      "Ground Truth: To address the latest noaa report is \u201ca reminder that climate change has not, despite the insistence of climate contrarians \u2018paused\u2019 or even slowed down,\u201d mann said.., the following approach can be implemented: 1. Develop policies to protect ecosystems, such as establishing protected areas and regulating resource extraction. 2. Promote sustainable practices among communities through education and incentives. 3. Invest in research to better understand the issue and develop innovative solutions. 4. Foster international cooperation to address global aspects of the problem. The term is also used within the context of global warming to describe sudden climate change that is detectable over the time-scale of a human lifetime, possibly as the result of feedback loops within the climate system. \"Details the findings of a new report that confirms NOAA data about climate change.\" Such disagreements are not new but the emails provided climate sceptics, in the sense of deniers or contrarians, with a golden opportunity to mount a sustained effort aimed at demonstrating the legitimacy of their views. I shall use \"climate sceptics\" here in the sense of \"climate deniers\", although there are obvious differences between scepticism and denial (see Shermer, 2010; Kemp, et al., 2010). The statement references the IPCC's Fourth Assessment of 2007, and asserts that \"climate change is happening even faster than previously estimated; global CO 2 emissions since 2000 have been higher than even the highest predictions, Arctic sea ice has been melting at rates much faster than predicted, and the rise in the sea level has become more rapid\". This approach seeks to balance environmental protection with sustainable development, ensuring long-term benefits for both nature and society.\n",
      "\n",
      "Problem: problem: Barack Obama supports proposals \"to devote billions of dollars annually to state game and fish agencies and federal land management agencies to help them ensure that fish and wildlife survive the impacts of climate change.\"\n",
      "Generated Approach (Standard): To address the issue of president obama supports proposals \"to devote billions of dollars annually to state game and fish agencies and federal land management agencies to help them ensure that fish and wildlife survive the impacts of climate change.\", the following approach can be implemented: 1. Develop policies to protect ecosystems, such as establishing protected areas and regulating resource extraction. 2. Promote sustainable practices among communities through education and incentives. 3. Invest in research to better understand the issue and develop innovative solutions. 4. Foster international cooperation to address global aspects of the problem. Obama supports proposals \"to allocate billions of dollars annually to the federal government to protect ecosystems, such as ensuring a unified approach to tackling this issue. Obama supports proposals to allocate billions of dollars to federal land management agencies and federal land management agencies, ensuring a unified approach to tackling this issue. This approach seeks to balance environmental protection with sustainable development, ensuring long-term benefits for both nature and society.\n",
      "Generated Approach (With Keywords): To address the issue of president obama supports proposals \"to devote billions of dollars annually to state game and fish agencies and federal land management agencies to help them ensure that fish and wildlife survive the impacts of climate change.\", the following approach can be implemented: 1. Develop policies to protect ecosystems, such as establishing protected areas and regulating resource extraction. 2. Promote sustainable practices among communities through education and incentives. 3. Invest in research to better understand the issue and develop innovative solutions. 4. Foster international cooperation to address global aspects of the problem. \"The impact of climate change is a major factor in the impact of climate change on ecosystems and ecosystems, and the impact of climate change on ecosystems and ecosystems. This approach seeks to balance environmental protection with sustainable development with sustainable development, ensuring long-term benefits for both nature and society. \"Impact climate change, climate change, and the impact of climate change on ecosystems and ecosystems, ensuring long-term benefits for both nature and society. \"This approach seeks to balance environmental protection with sustainable development, ensuring long-term benefits for both nature and society.\n",
      "Ground Truth: To address barack obama supports proposals \"to devote billions of dollars annually to state game and fish agencies and federal land management agencies to help them ensure that fish and wildlife survive the impacts of climate change.\", the following approach can be implemented: 1. Develop policies to protect ecosystems, such as establishing protected areas and regulating resource extraction. 2. Promote sustainable practices among communities through education and incentives. 3. Invest in research to better understand the issue and develop innovative solutions. 4. Foster international cooperation to address global aspects of the problem. He sponsored the Iran Sanctions Enabling Act supporting divestment of state pension funds from Iran's oil and gas industry, which was never enacted but later incorporated in the Comprehensive Iran Sanctions, Accountability, and Divestment Act of 2010; and co-sponsored legislation to reduce risks of nuclear terrorism. His proposal was to spend $900 billion over 10 years and include a government insurance plan, also known as the public option, to compete with the corporate insurance sector as a main component to lowering costs and improving quality of health care. They include: stewardship and education; professional excellence; youth programs; climate change impacts; and budget restructure and realignment. The budget includes $2.4 billion for Wildland Fire Management, a decrease of $530 million from the 2019 Annualized Continuing Resolution because the \u201cfire fix\u201d cap adjustment becomes available in FY 2020, while the FY 2019 Annualized Continuing Resolution includes $500 million above the base as bridge to the first year of the fire fix. Through implementation of land and resource management plans, the agency ensures sustainable ecosystems by restoring and maintaining species diversity and ecological productivity that helps provide recreation, water, timber, minerals, fish, wildlife, wilderness, and aesthetic values for current and future generations of people. This approach seeks to balance environmental protection\n",
      "\n",
      "Problem: problem: As a result, half of the people surveyed around the world last year said they thought climate change would make humanity extinct.\n",
      "Generated Approach (Standard): To address as a result, half of the people surveyed around the world last year said they thought climate change would make humanity extinct., the following approach can be implemented: 1. Develop policies to protect ecosystems, such as establishing protected areas and regulating resource extraction. 2. Promote sustainable practices among communities through education and incentives. 3. Invest in research to better understand the issue and develop innovative solutions. 4. Foster international cooperation to address global aspects of the problem. In addition, half of the people surveyed around the world last year said they thought climate change would make humanity extinct. In a survey, half of the people surveyed around the world said they thought climate change would make humanity extinct. In the United States, a unified approach can be implemented: 1. Develop policies to protect ecosystems, such as establishing protected areas and ensuring long-term benefits for both nature and society. In the United States, a unified approach to tackling this issue. This approach seeks to balance environmental protection with sustainable development, ensuring long-term benefits for both nature and society.\n",
      "Generated Approach (With Keywords): To address as a result, half of the people surveyed around the world last year said they thought climate change would make humanity extinct., the following approach can be implemented: 1. Develop policies to protect ecosystems, such as establishing protected areas and regulating resource extraction. 2. Promote sustainable practices among communities through education and incentives. 3. Invest in research to better understand the issue and develop innovative solutions. 4. Foster international cooperation to address global aspects of the problem. The following approach can be implemented: 1. Develop policies to protect ecosystems, such as establishing protected areas and ensuring a unified approach to tackling this issue. In the United States, the United States, the United States, the United States, and the United States, ensuring a unified approach to tackling this issue. This approach seeks to balance environmental protection with sustainable development, ensuring long-term benefits for both nature and society.\n",
      "Ground Truth: To address as a result, half of the people surveyed around the world last year said they thought climate change would make humanity extinct., the following approach can be implemented: 1. Develop policies to protect ecosystems, such as establishing protected areas and regulating resource extraction. 2. Promote sustainable practices among communities through education and incentives. 3. Invest in research to better understand the issue and develop innovative solutions. 4. Foster international cooperation to address global aspects of the problem. (2007:788) concluded that a global mean temperature increase of around 4\u00a0\u00b0C (above the 1990-2000 level) by 2100 would lead to major extinctions around the globe. \"Biologists think 50% of species will be facing extinction by the end of the century\". A 2016 survey found that two-thirds of AMS members think that all or most of climate change is caused by human activity. Catastrophic effects in 50\u2013100 years would likely be observed according to 41%, while 44% thought the effects would be moderate and about 13 percent saw relatively little danger. Among all respondents, 90% agreed that temperatures have risen compared to pre-1800 levels, and 82% agreed that humans significantly influence the global temperature. This approach seeks to balance environmental protection with sustainable development, ensuring long-term benefits for both nature and society.\n",
      "\n",
      "Critical Analysis Prompts for Your Report:\n",
      "1. Dataset Bias:\n",
      "   - Did the ClimateFever dataset overrepresent certain types of climate-related problems (e.g., carbon emissions) and underrepresent others (e.g., biodiversity)?\n",
      "   - How did the synthesized approaches impact the model\u2019s outputs? Were they too generic due to the templating approach?\n",
      "2. Model Performance:\n",
      "   - How did the optimized hyperparameters improve performance compared to default settings? Compare ROUGE-L and BERTScore.\n",
      "   - Did the model generate feasible approaches, or were there vague/incorrect suggestions (e.g., impractical solutions)?\n",
      "   - Did adding keywords to the input improve the quality of generated approaches? Why or why not?\n",
      "3. Hyperparameter Optimization:\n",
      "   - What did you learn from the Optuna search? For example, did a smaller learning rate or more epochs lead to better performance?\n",
      "   - Were there any trade-offs (e.g., longer training time vs. better performance)?\n",
      "4. Ethical Issues:\n",
      "   - Could the model propagate misinformation if the synthesized approaches oversimplify complex environmental problems?\n",
      "   - What are the implications of using this system in real-world environmental research? How might incorrect approaches impact policy or action?\n"
     ]
    }
   ]
  }
 ]
}